{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project develops a data model based on immigration data for immigration into the US and develops tables to be used for analysis.\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The scope of this project is to provide 2 analytical tables to be used for analysis on US demographics and immigration. One table will focus on individual immigration records, the other will contain US city demographic information. New columns will be created to allow comparison of median age of residents and median age of visitors to a state. Additionally a new column will be created that shows count of visitors to a state, so analysis can be performed on the most popular and least popular states and how this compares to overall population.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "In this project we will use 3 different data sources.  \n",
    "Our first dataset contains I94 immigration data which has details of any immigration into the US. This includes people permanently moving and just visiting the US.   \n",
    "The second dataset we will use contains US city demographic data.  \n",
    "Our final data source is a json file containing the 2 letter state code and the full name of the state it corresponds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "immigration_data_df=pd.read_csv(\"immigration_data_sample.csv\")\n",
    "us_cities_df=pd.read_csv(\"us-cities-demographics.csv\", delimiter=';')\n",
    "code_state_df=pd.read_json(\"code_state.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>ALABAMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AZ</td>\n",
       "      <td>ARIZONA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR</td>\n",
       "      <td>ARKANSAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Code       State\n",
       "0   AL     ALABAMA\n",
       "1   AK      ALASKA\n",
       "2   AZ     ARIZONA\n",
       "3   AR    ARKANSAS\n",
       "4   CA  CALIFORNIA"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view code state mapping json\n",
    "code_state_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view us cities demographic data\n",
    "us_cities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unneeded columns from us cities data\n",
    "us_cities_df.drop(['Race', 'Count', 'Number of Veterans', 'Foreign-born', 'Average Household Size'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "\n",
       "   i94mode i94addr    ...     entdepu  matflag  biryear   dtaddto  gender  \\\n",
       "0      1.0      HI    ...         NaN        M   1955.0  07202016       F   \n",
       "1      1.0      TX    ...         NaN        M   1990.0  10222016       M   \n",
       "2      1.0      FL    ...         NaN        M   1940.0  07052016       M   \n",
       "3      1.0      CA    ...         NaN        M   1991.0  10272016       M   \n",
       "4      3.0      NY    ...         NaN        M   1997.0  07042016       F   \n",
       "\n",
       "  insnum airline        admnum  fltno  visatype  \n",
       "0    NaN      JL  5.658267e+10  00782        WT  \n",
       "1    NaN     *GA  9.436200e+10  XBLNG        B2  \n",
       "2    NaN      LH  5.578047e+10  00464        WT  \n",
       "3    NaN      QR  9.478970e+10  00739        B2  \n",
       "4    NaN     NaN  4.232257e+10   LAND        WT  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view sample of immigration data\n",
    "immigration_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up Spark session\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "# read in one of the files as a spark dataframe\n",
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat',\n",
       " '../../data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat',\n",
       " '../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat',\n",
       " '../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat',\n",
       " '../../data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat',\n",
       " '../../data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat',\n",
       " '../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat',\n",
       " '../../data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat',\n",
       " '../../data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat',\n",
       " '../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat',\n",
       " '../../data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list of all .sas7bdat files\n",
    "all_files = glob.glob('../../data/18-83510-I94-Data-2016/*.sas7bdat')\n",
    "# drop the first file as it has already been used to create df_spark\n",
    "all_files.remove('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['../../data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat', 28, True],\n",
       " ['../../data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat', 28, True],\n",
       " ['../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat', 28, True],\n",
       " ['../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat', 34, False],\n",
       " ['../../data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat', 28, True],\n",
       " ['../../data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat', 28, True],\n",
       " ['../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat', 28, True],\n",
       " ['../../data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat', 28, True],\n",
       " ['../../data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat', 28, True],\n",
       " ['../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat', 28, True],\n",
       " ['../../data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat', 28, True]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check all files have the same number and names of columns\n",
    "columns=[]\n",
    "for fname in all_files:\n",
    "    df = spark.read.format('com.github.saurfang.sas.spark').load(fname)\n",
    "    equal_columns = (df.columns == df_spark.columns)\n",
    "    columns.append([fname,len(df.columns), equal_columns])\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cicid',\n",
       " 'i94yr',\n",
       " 'i94mon',\n",
       " 'i94cit',\n",
       " 'i94res',\n",
       " 'i94port',\n",
       " 'arrdate',\n",
       " 'i94mode',\n",
       " 'i94addr',\n",
       " 'depdate',\n",
       " 'i94bir',\n",
       " 'i94visa',\n",
       " 'count',\n",
       " 'validres',\n",
       " 'delete_days',\n",
       " 'delete_mexl',\n",
       " 'delete_dup',\n",
       " 'delete_visa',\n",
       " 'delete_recdup',\n",
       " 'dtadfile',\n",
       " 'visapost',\n",
       " 'occup',\n",
       " 'entdepa',\n",
       " 'entdepd',\n",
       " 'entdepu',\n",
       " 'matflag',\n",
       " 'biryear',\n",
       " 'dtaddto',\n",
       " 'gender',\n",
       " 'insnum',\n",
       " 'airline',\n",
       " 'admnum',\n",
       " 'fltno',\n",
       " 'visatype']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# june 2016 data has different columns\n",
    "df_jun16 = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat')\n",
    "df_jun16.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of relevant columns\n",
    "df_columns=['cicid',\n",
    " 'i94yr',\n",
    " 'i94mon',\n",
    " 'i94cit',\n",
    " 'i94res',\n",
    " 'i94port',\n",
    " 'arrdate',\n",
    " 'i94addr',\n",
    " 'depdate',\n",
    " 'i94bir',\n",
    " 'i94visa',\n",
    " 'count',\n",
    " 'matflag',\n",
    " 'biryear',\n",
    " 'gender',\n",
    " 'visatype']\n",
    "# filter spark dataframe\n",
    "df_spark_filtered=df_spark.select(df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# union all immigration data files, selecting only the relevant columns to reduce memory storage\n",
    "for fname in all_files:\n",
    "    df = spark.read.format('com.github.saurfang.sas.spark').load(fname)\n",
    "    df_2=df.select(df_columns)\n",
    "    df_filtered=df_spark_filtered.union(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6529303"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of immigration records\n",
    "df_filtered.count()\n",
    "# over 1 million records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "df_filtered.write.parquet(\"parquet_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in parquet data\n",
    "df_filtered_pq=spark.read.parquet(\"parquet_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for null values and remove columns with over 50% null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for columns which contain > 50% null values\n",
    "nulls_list=[]\n",
    "for col in df_filtered_pq.columns:\n",
    "    count_nulls=df_filtered_pq.filter(F.col(col).isNull()).count()\n",
    "    nulls_list.append([col, \"No of nulls: \" +str(count_nulls)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3264651.5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get total number of rows\n",
    "df_filtered_pq_count=df_filtered_pq.count()\n",
    "# get value for 50% of rows\n",
    "df_filtered_pq_count*0.5\n",
    "# 50 % is equal to 3264651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cicid', 'No of nulls: 0'],\n",
       " ['i94yr', 'No of nulls: 0'],\n",
       " ['i94mon', 'No of nulls: 0'],\n",
       " ['i94cit', 'No of nulls: 969'],\n",
       " ['i94res', 'No of nulls: 0'],\n",
       " ['i94port', 'No of nulls: 0'],\n",
       " ['arrdate', 'No of nulls: 0'],\n",
       " ['i94addr', 'No of nulls: 319417'],\n",
       " ['depdate', 'No of nulls: 357245'],\n",
       " ['i94bir', 'No of nulls: 1495'],\n",
       " ['i94visa', 'No of nulls: 0'],\n",
       " ['count', 'No of nulls: 0'],\n",
       " ['matflag', 'No of nulls: 342281'],\n",
       " ['biryear', 'No of nulls: 1495'],\n",
       " ['gender', 'No of nulls: 481875'],\n",
       " ['visatype', 'No of nulls: 0']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show nulls_list and look for any columns where nulls are above 3264651\n",
    "nulls_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns that contain over 50% null values:\n",
    "- depdate \n",
    "- matflag \n",
    "- gender  \n",
    "\n",
    "*depdate* represents departure date. Where it is null may just been the person has moved\n",
    "to the US permanently or just not yet departed, so we will not drop this column.  \n",
    "*matflag* shows a match between arrdate and depdate, it does not provide much information so can be dropped.  \n",
    "*gender* does not contain enough data to be used for useful analysis so will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop redundant columnns\n",
    "df_dropped=df_filtered_pq.drop('matflag').drop('gender')\n",
    "# drop duplicates\n",
    "df_dropped=df_dropped.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure data contains a unique ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count of rows in the dataframe is: 6529303. The count of distinct cicid values is: 5127515.\n"
     ]
    }
   ],
   "source": [
    "# check cicid column is unique\n",
    "df_dropped_count=df_dropped.count()\n",
    "cicid_unique_count=df_dropped.select('cicid').distinct().count()\n",
    "print(\"The count of rows in the dataframe is: \" + str(df_dropped_count) + \". The count of distinct cicid values is: \"\n",
    "     + str(cicid_unique_count) +\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since cicid is not unique we will drop it as it is not able to serve as a unique id value. We will then create a new id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new id column and dropping cicid column\n",
    "df_dropped=df_dropped.withColumn('id', F.row_number().over(Window.orderBy(F.monotonically_increasing_id()))\n",
    "                                ).drop('cicid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename and re-format columns\n",
    "immigration_data=df_dropped.select(\n",
    "        F.col('id'),\n",
    "        F.col('i94yr').alias('record_year'),\n",
    "        F.col('i94mon').alias('record_month'),\n",
    "        F.col('i94cit').alias('origin_location'),\n",
    "        F.col('i94res').alias('resident_location'),\n",
    "        F.col('i94port').alias('airport_code'),\n",
    "        F.col('arrdate').alias('arrival_date'),\n",
    "        F.col('i94addr').alias('destination'),\n",
    "        F.col('depdate').alias('departure_date'),\n",
    "        F.col('i94bir').alias('age'),\n",
    "        F.col('visatype').alias('visa_type')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference date for SAS (January 1, 1960)\n",
    "sas_reference_date = \"1960-01-01\"\n",
    "\n",
    "# convert departure and arrival date from SAS date types\n",
    "immigration_data = immigration_data.withColumn(\n",
    "    \"arrival_date\", F.expr(f\"date_add('{sas_reference_date}', arrival_date)\")).withColumn(\n",
    "    \"departure_date\", F.expr(f\"date_add('{sas_reference_date}', departure_date)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform transformations on the us_cities dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat columns, putting all columns in lowercase and removing any whitespace\n",
    "us_cities_df.columns=us_cities_df.columns.str.lower().str.replace(' ', '_')\n",
    "us_cities_df.columns=us_cities_df.columns.str.lower().str.replace('-', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capitalise and trim city and state names\n",
    "us_cities_df['city']=us_cities_df['city'].str.upper()\n",
    "us_cities_df['state']=us_cities_df['state'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a unique id column based on city and state\n",
    "us_cities_df['city_state_id']=us_cities_df.state_code.str.lower()+'_'+us_cities_df.city.str.lower().str.replace(' ', '_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename median_age column\n",
    "us_cities_df=us_cities_df.rename(columns={'median_age':'median_resident_age'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate values in US cities data\n",
    "us_cities=us_cities_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change order of columns\n",
    "ordered_list=[ 'city_state_id','city', 'state', 'state_code', 'male_population', 'female_population',\n",
    " 'total_population','median_resident_age']\n",
    "us_cities=us_cities[ordered_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformat column names in state_code dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all columns in lowercase and remove any whitespace\n",
    "code_state_df.columns=code_state_df.columns.str.lower().str.replace(' ', '_')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "**Steps:**\n",
    "1. Group *immigration_data* by *destination* and calculate the *median_visitor_age* as a new column.  \n",
    "2. Left join *us_cities* with *immigration_data*, to get all columns in us_cities plus just the *median_visitor_age* column which is created from data from *immigration_data*. Join using *us_cities.state_code* and *immigration_data.destination* columns.\n",
    "3. Calculate total sum of visitors by *destination*, creating a new column *count_of_visitors* on the dataframe *immigration_data*.\n",
    "4. Join *immigration_data* with *code_state* to get the full names of the states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate median age of visitors using the immigration_data dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate median_age of visitors\n",
    "# define a function to calculate the median\n",
    "def calculate_median(df, group_col, target_col):\n",
    "    # Collect the median values as a list of tuples\n",
    "    median_values = df.groupBy(group_col).agg(\n",
    "        F.expr(f'percentile_approx({target_col}, 0.5)').alias('median_visitor_age')\n",
    "    ).collect()\n",
    "    \n",
    "    # Convert the list of tuples to a dictionary for easy access\n",
    "    median_dict = {row[group_col]: row['median_visitor_age'] for row in median_values}\n",
    "    return median_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the median age by destination\n",
    "median_age_by_destination = calculate_median(immigration_data, 'destination', 'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the dictionary to a pandas DataFrame\n",
    "median_age_df = spark.createDataFrame(\n",
    "    [(k, v) for k, v in median_age_by_destination.items()],\n",
    "    [ 'destination', 'median_visitor_age']    \n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Add *median_visitor_age* to *us_cities* dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join median_age_df with us_cities\n",
    "us_cities=us_cities.merge(median_age_df, left_on='state_code', right_on='destination', how='left').drop(\n",
    "    'destination', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Calculate the total sum of visitors by *destination*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the count of visitors by destination\n",
    "count_of_visitors= immigration_data.select(\n",
    "    F.col('id'), \n",
    "    F.col('destination').alias('visitor_destination')\n",
    ").groupBy('visitor_destination').agg(\n",
    "         F.count('id').alias('count_of_visitors'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join count_of_visitors with immigration_data\n",
    "immigration_data=immigration_data.join(count_of_visitors, \n",
    "                                       immigration_data.destination==count_of_visitors.visitor_destination, \n",
    "                                       how='left').drop('visitor_destination')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get full state names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark version of code_state_df\n",
    "code_state_spark=spark.createDataFrame(code_state_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join city_state with immigration_data\n",
    "immigration_data=immigration_data.join(code_state_spark, immigration_data.destination==code_state_spark.code,\n",
    "                                       how='left'\n",
    "                                      ).withColumnRenamed('state', 'destination_name').drop('code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a test to ensure the table contains data\n",
    "def check_for_records(df):\n",
    "    ''' \n",
    "    Checks that a pandas dataframe contains data by checking that the number of records is at least 1.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas dataframe\n",
    "    Returns:\n",
    "        Error message if df contains no records, else nothing is returned\n",
    "    '''\n",
    "    if len(df) < 1:\n",
    "        return 'Error: the dataframe ' + str(df) +' does not contain any records.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run test\n",
    "check_for_records(us_cities)\n",
    "# test passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test to check expected column headers in us_cities (checking they have been reformatted)\n",
    "def column_check(df, expected_columns):\n",
    "    '''\n",
    "    Checks column names of a dataframe against a list of expected column names. \n",
    "    Columns must be in the same order for the test to pass.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas dataframe\n",
    "        expected_columns: list of expected columns in the desired order, column names will be case sensitive. \n",
    "    Returns:\n",
    "        AssertionError if expected columns do not match df columns\n",
    "    '''\n",
    "    assert df.columns.tolist()==expected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run test\n",
    "expected_us_cities_columns=['city_state_id','city', 'state', 'state_code', 'male_population', 'female_population',\n",
    "       'total_population', 'median_resident_age','median_visitor_age']\n",
    "column_check(us_cities, expected_us_cities_columns)\n",
    "# test passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All tests passed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionarys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Table | Column name | Description|\n",
    "| --- | --- | --- |\n",
    "| us_cities | city_state_id | Unique identifier made up of state code and city name |\n",
    "| us_cities | city | US city name |\n",
    "| us_cities | state | US state full name |\n",
    "| us_cities | state_code | US state 2 letter code |\n",
    "| us_cities | male_population | Male population volume for the city |\n",
    "| us_cities | female_population | Female population volume for the city |\n",
    "| us_cities | total_population | Total population volume for the city |\n",
    "| us_cities | median_age | Median age of residents in the city |\n",
    "| us_cities | median_visitor_age | Median age of visitors to the state | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Table | Column name | Description|\n",
    "| --- | --- | --- |\n",
    "| immigration_data | id | Unique identifier |\n",
    "| immigration_data | record_year | Year that the data was recorded |\n",
    "| immigration_data | record_month | Month that the data was recorded represented as a number 1-12 |\n",
    "| immigration_data | origin_location | Location the person flew in from |\n",
    "| immigration_data | resident_location | Location the person is legally a resident of |\n",
    "| immigration_data | airport_code | Code of the aiport they flew to |\n",
    "| immigration_data | arrival_date | Date of arrival into the US |\n",
    "| immigration_data | destination | Destination of the person given as US state 2 letter code |\n",
    "| immigration_data | departure_date | Data of departure from the US |\n",
    "| immigration_data | age | Age in years |\n",
    "| immigration_data | visa_type | Type of visa |\n",
    "| immigration_data | median_visitor_age | Median age of visitors to the destination |\n",
    "| immigration_data | count_of_visitors | Count of visitors by destination |\n",
    "| immigration_data | destination_name | Name of destination state |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project write up\n",
    "I used Spark for the immigration data as this was a really large dataset with over 6,000,000 rows. One of the main benefits of using Spark for this data is that Sparks execution engine will optimise the execution plan and its in-memory computation will increase processing speed. Due to the distributed computing element of Spark it can process data in parallel across a cluster of machines so it can handle this large dataset efficiently.\n",
    "For our smaller datasets such as US city demographic data I used Pandas, as I didn't need to utilise the distributed computing or scalability aspects of Spark it was more compute efficient to use Pandas. \n",
    "\n",
    "The data should be updated monthly using Batch processing. It does not need to be updated live as we are interested in using the data for retrospective analysis. A monthly frequency will be sufficient for analysis. \n",
    "\n",
    "- If the data was increased by 100x I would use partitioning in Spark to partition data so it can be ran in parallel across multiple clusters. I would partition by destination.  \n",
    "- If the data populated a dashboard that must be updated on a daily basis by 7am every day I would first increase the frequency that data is updated, from monthly to daily, updating at midnight. I would schedule the update of the data using Airflow and a daily schedule. I would configure the DAG using an SLA to complete by 7am every day.  \n",
    "- If the data needed to be accessed by 100+ people I would host it in Snowflake to allow for a user friendly experience that allows multiple users to access the same dataset at the same time. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
